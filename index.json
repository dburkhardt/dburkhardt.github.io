[{"authors":null,"categories":null,"content":"As single cell RNA-sequencing experiments become more popular, we keep on hearing a few questions over and over: \u0026ldquo;How should I start analyzing my data?\u0026rdquo; \u0026ldquo;What\u0026rsquo;s the advantage of using PHATE over t-SNE?\u0026rdquo; \u0026ldquo;How should I cluster my data?\u0026rdquo; \u0026ldquo;How can I identify differentially expressed genes between these two clusters?\u0026rdquo;\nThis tutorial is designed to answer those questions.\nSections:\n1. Introduction\n2. Preprocessing\n3. Visualization using PCA\n","date":1536465600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1546318800,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"https://dburkhardt.github.io/tutorial/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/tutorial/","section":"tutorial","summary":"As single cell RNA-sequencing experiments become more popular, we keep on hearing a few questions over and over: \u0026ldquo;How should I start analyzing my data?\u0026rdquo; \u0026ldquo;What\u0026rsquo;s the advantage of using PHATE over t-SNE?\u0026rdquo; \u0026ldquo;How should I cluster my data?\u0026rdquo; \u0026ldquo;How can I identify differentially expressed genes between these two clusters?\u0026rdquo;\nThis tutorial is designed to answer those questions.\nSections:\n1. Introduction\n2. Preprocessing\n3. Visualization using PCA","tags":null,"title":"How to single cell","type":"docs"},{"authors":null,"categories":null,"content":"As single cell RNA-sequencing experiments become more popular, we keep on hearing a few questions over and over: \u0026ldquo;How should I start analyzing my data?\u0026rdquo; \u0026ldquo;What\u0026rsquo;s the advantage of using PHATE over t-SNE?\u0026rdquo; \u0026ldquo;How should I cluster my data?\u0026rdquo; \u0026ldquo;How can I identify differentially expressed genes between these two clusters?\u0026rdquo;\nHigh throughput technologies are only a few years old, and there are not many (if any) standards for analyzing a dataset. This being said, there are a few nice tutorials out there. Rahul Satija\u0026rsquo;s lab at the NY Genome Center, the home of the popular Seurat toolkit has a collection of easy-to-follow tutorials. In May of this year, the Hemberg lab at the Sanger Institute posted a great single cell course as part of the Bioinformatics Training Unit at Cambridge. John Marioni\u0026rsquo;s lab at UCL has a paper on F1000 on Low-level analysis of single-cell RNA-seq data with Bioconductor. A quick google search will reveal a handful of other tutorials.\nI encourage reading widely and getting a sense of what other groups do. There are many ways to do single cell analysis. Some ways are definitely bad (don\u0026rsquo;t get us started on clustering on t-SNE dimensions), but usually the important thing is trying to understand how the methods you apply work and why you might pick one tool or another. In general, you should view the results of most of these analyses as hypotheses that need to be validated by the specific tools of your discipline.\nThis being said, in the Krishnaswamy lab, we have gone through hundreds of samples of scRNA-seq data and have learned a lot along the way. We have developed a growing number of novel methods for analyzing these datasets and extracting biological meaning. You can find all of our tools publicly available on our lab github: https://github.com/krishnaswamylab/.\nIn this introductory post, my goal is to go through the basics of analyzing a single cell RNA-sequencing dataset composed of a few samples. I\u0026rsquo;m assuming you already know what scRNA-seq is and have read a few papers in the field. I\u0026rsquo;m also assuming you\u0026rsquo;ve heard of Python and are willing to learn Numpy, Pandas, and Matplotlib. Most of the tools in the lab have been ported to R and are available as part of the scanpy and seurat packages, but all of us in the lab use python when we\u0026rsquo;re analyzing datasets.\nHere\u0026rsquo;s the basic workflow we\u0026rsquo;re going to cover in this post:\n1. Using the scprep toolkit for loading your data\n2. Filtering cells based on library size\n3. Filtering lowly expressed genes\n4. Examining expression of mitochondrial RNA\n5. Visualizing data using PCA\n6. Visualizing data using PHATE\n7. Imputing data using MAGIC\n8. Examining the expression of marker genes\nWe already have a set of tutorials for some of these tools on our lab Github: www.github.com/krishnaswamylab but here I will take a higher level approach to starting analysis and provide some insights that will hopefully facilitate your analysis.\n","date":1536465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546318800,"objectID":"8462514b96e0b7c6720a1d9ad2354c7d","permalink":"https://dburkhardt.github.io/tutorial/introduction/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/tutorial/introduction/","section":"tutorial","summary":"As single cell RNA-sequencing experiments become more popular, we keep on hearing a few questions over and over: \u0026ldquo;How should I start analyzing my data?\u0026rdquo; \u0026ldquo;What\u0026rsquo;s the advantage of using PHATE over t-SNE?\u0026rdquo; \u0026ldquo;How should I cluster my data?\u0026rdquo; \u0026ldquo;How can I identify differentially expressed genes between these two clusters?\u0026rdquo;\nHigh throughput technologies are only a few years old, and there are not many (if any) standards for analyzing a dataset.","tags":null,"title":"Introduction","type":"docs"},{"authors":null,"categories":null,"content":" 1.0 Introducing scprep Scprep is a lightweight scRNA-seq toolkit for Python Data Scientists Most scRNA-seq toolkits are written in R, but we develop our tools in Python. Currently, Scanpy is the most popular toolkit for scRNA-seq analysis in Python. In fact, if you\u0026rsquo;d prefer to use that, you can find most of our lab\u0026rsquo;s analytical methods including PHATE and MAGIC in scanpy. However, scanpy has a highly structured framework for data representation and a steep learning curve that is unnescessary for users already comfortable with the suite of methods available in Pandas, Scipy, and Sklearn.\nTo accomodate these users (including ourselves) we developed scprep (single cell preparation). scprep makes it easier to use the Pandas / Scipy / Sklearn ecosystem for scRNA-seq analysis. Most of scprep is composed of helper functions to perform tasks common to single cell data like loading counts matrices, filtering \u0026amp; normalizing cells by library size, and calculating common statistics. The key advantage of scprep is that data can be stored in Pandas DataFrames, Numpy arrays, Scipy Sparse Matices, it is just works.\nFor users starting out, you might find it more valuable to spend time spent getting comfortable with these tools because they can be used for analysis of all kinds of data, not just scRNA-seq. If you want to learn more, checkout the Numpy Quickstart Tutorial, the Pandas Tutorials, the Matplotlib Tutorials, and consider getting a copy of Wes McKinney\u0026rsquo;s book Python for Data Analysis. McKinney is the original author of Pandas, and I\u0026rsquo;m glad I went through the book at the beginning of my PhD.\nInstallation From here on out, I will assume that you have Python, Numpy, Pandas, Scipy, Scikit-learn, and Matplotlib all installed.\nThe scprep package is available on GitHub and on PyPI so the install is straightforward:\n$ pip install --user scprep In scprep you\u0026rsquo;ll find tools for loading single cell data (scprep.io), library size normalization (scprep.normalize), transforming data (scprep.transform), performing statistical calculations (scprep.stats), and many others. Check out the screp documentation for the full API.\n1.1 - Using scprep to load a 10X Genomics counts matrix Most of the labs we work with use 10X Genomics for single cell sequencing, so we\u0026rsquo;ll use this as an example.\nscprep.io also has helper functions for csv, tsv, and mtx files.\n Say your 10X data is in /home/user/data/my_sample. To load your data, run:\n1 2  import scprep data_uf = scprep.io.load_10X(\u0026#39;/home/user/data/my_sample/outs/filtered_gene_bc_matrices/my_genome/\u0026#39;, sparse=True, gene_labels=\u0026#39;both\u0026#39;)   Here, data_uf is my shorthand for \u0026lsquo;data_unfiltered\u0026rsquo;. load_10X and other scprep.io functions return a Pandas SparseDataFrame by default. The SparseDataFrame behaves similarly to regular Pandas DataFrames, but take up much less memory by storing only non-zero values and their indices in memory. Note: operations are sparse matrices are slower than dense matrices. If you can afford to store the dense matrix in memory, set sparse=False.\nAs an aside, many genome annotations (the descriptions of where genes are located on a given genome) contain non-unique gene symbols, so with gene_labels=both we store the gene symbol and the gene ID as 'ACTB (ENSG00000075624)'.\nIf you look at the head of your data, you should see something like this:\ndata_uf.head()  [sparse_dataframe_head.png]\nNow, let\u0026rsquo;s start preprocessing your data.\n1.2 - Merging batches using scprep (optional) This is the point to merge batches if you have multiple samples from the sample experiment that you want to compare. Although we\u0026rsquo;ll talk briefly about batch effects later, we\u0026rsquo;ll write a whole post about how to correct them at another point.\nHow to combine samples Here, what we\u0026rsquo;re doing is taking counts matrices from each sample and stacking them vertically. We\u0026rsquo;ve implemented this in the scprep.utils.combine_batches function.\nFor this example, I\u0026rsquo;m going to use the Embryoid Body timecourse covered in the PHATE tutorial.\nTwo important notes:\nFirst, it\u0026rsquo;s important to know exactly which rows of the data matrix correspond to which sample so that we can separate them during downstream analysis. To facilitate this, combine_batches() takes an list batch_labels that contains one label per sample and returns an array sample labels that contains the sample label for each row in the resulting matrix.\nSecond, it\u0026rsquo;s possible (and exceedingly common) for the same cell barcode to appear in multiple experiments. This becomes an issue when combining batches because you want each row to have a unique index. To solve this problem, we append the sample label to the cell barcode by setting append_to_cell_names=True. This turns cell barcode AAACATACCAGAGG-1 from sample Day0-3 to AAACATACCAGAGG-1_Day0-3\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  # Loading each of 5 timepoints T1 = scprep.io.load_10X(os.path.join(download_path, \u0026#34;scRNAseq\u0026#34;, \u0026#34;T0_1A\u0026#34;), gene_labels=\u0026#39;both\u0026#39;) T2 = scprep.io.load_10X(os.path.join(download_path, \u0026#34;scRNAseq\u0026#34;, \u0026#34;T2_3B\u0026#34;), gene_labels=\u0026#39;both\u0026#39;) T3 = scprep.io.load_10X(os.path.join(download_path, \u0026#34;scRNAseq\u0026#34;, \u0026#34;T4_5C\u0026#34;), gene_labels=\u0026#39;both\u0026#39;) T4 = scprep.io.load_10X(os.path.join(download_path, \u0026#34;scRNAseq\u0026#34;, \u0026#34;T6_7D\u0026#34;), gene_labels=\u0026#39;both\u0026#39;) T5 = scprep.io.load_10X(os.path.join(download_path, \u0026#34;scRNAseq\u0026#34;, \u0026#34;T8_9E\u0026#34;), gene_labels=\u0026#39;both\u0026#39;) # Concatenating data matrices EBT_counts, sample_labels = scprep.utils.combine_batches( data=[T1, T2, T3, T4, T5], batch_labels=[\u0026#34;Day0-3\u0026#34;, \u0026#34;Day6-9\u0026#34;, \u0026#34;Day12-15\u0026#34;, \u0026#34;Day18-21\u0026#34;, \u0026#34;Day24-27\u0026#34;], append_to_cell_names=True ) # remove objects from memory del T1, T2, T3, T4, T5    It\u0026rsquo;s good practice to remove the original data matrices from memory and avoid doubling the memory usage of our scripts.\n 1.3 - Filtering cells by library size Why we filter cells by library size In scRNA-seq the library size of a cell is the number of unique mRNA molecules detected in that cell. These unique molecules are identified using a random barcode incorporated during the first round of reverse transcription. This barcode is called a Unique Molecule Indicator, and often we refer to the number unique mRNAs in a cell as the number of UMIs. To read more about UMIs, Smith et al. (2017) write about how sequencing errors and PCR amplification errors lead to innaccurate quantification of UMIs/cell.\nDepending on the method of scRNA-seq, the amount of library size filtering done can vary. The 10X Genomics CellRanger tool, the DropSeq and InDrops pipelines, and the Umitools package each have their own method and cutoff for determining real cells from empty droplets. You can take these methods at face value or set some manual cutoffs based on your data.\nVisualing the library size distribution using scprep There is a helper function for plotting library size from a gene expression matrix in scprep called scprep.plot.plot_library_size().\nHere we can see a typical library size for a 10X dataset from Datlinger et al. (2017). In this sample we see that there is a long tail of cells that have very high library sizes.\nSelecting a cutoff Several papers describe strategies for picking a maximum and minimum threshold that can be found with a quick google search for \u0026ldquo;library size threshold single cell RNA seq\u0026rdquo;.\nMost of these pick an arbitrary measure such as a certain number of deviations below or above the mean or median library size. We find that spending too much time worrying about the exact threshold is inefficient.\nFor the above dataset, I would remove all cells with more than 25,000 UMI / cell in fear they might represent doublets of cells. I will generally also remove all cells with fewer than 500 reads per cell.\nFiltering cells by library size You can do this using scprep.filter.filter_library_size(). The syntax looks like:\ndata = scprep.filter.filter_library_size(data_uf, cutoff=25000, keep_cells=\u0026#39;below\u0026#39;) And now when we plot the library size we see:\nNote that there are many different valid distributions of library sizes. See the following three libraries, all generated from different tissues in the zebrafish embryo. One of the libraries is from a failed experiment, and the other two are from published papers.\nCan you tell which is which?\nHere, Sample 1 is from Pandley et al. (2018), Sample 2 is an internal failed experiment, and Sample 3 is from Farrell et al. (2018). The low library size in Sample 2 is the giveaway with 90% of cells having fewer than 1200 UMI/cell and a mode at 325 UMI/cell. Additionally, this library generated a Low Fraction Reads in Cells alert in the Cell Ranger web summary with only 33% of reads assigned to cells. To learn more about what this means, read this post in the 10X Forums..\n1.4 - Filtering lowly expressed genes Why remove lowly expressed genes? Capturing RNA from single cells is a noisy process. The first round of reverse transcription is done in the presence of cell lysate. This results in capture of only 10-40% of the mRNA molecules in a cell leading to a phenomenon called dropout where some lowly expressed genes are not detected in cells in which they are expressed [1, 2, 3, 4]. As a result, some genes are so lowly expressed (or expressed not at all) that we do not have sufficient observations of that gene to make any inferences on its expression.\nLowly expressed genes that may only be represented by a handful of mRNAs may not appear in a given dataset. Others might only be present in a small number of cells. Because we lack sufficient information about these genes, we remove lowly expressed genes from the gene expression matrix during preprocessing. Typically, if a gene is detected in fewer than 5 or 10 cells, it gets removed.\nHere, we can see that in the T cell dataset from Datlinger et al. (2017), there are many genes that are detected in very few cells.\n1 2 3 4 5 6 7 8 9  genes_per_cell = np.sum(t_cell_data \u0026gt; 0, axis=0) fig, ax = plt.subplots(1, figsize=(4,6)) ax.hist(genes_per_cell, bins=100) ax.set_xlabel(\u0026#39;# cells in which gene is expressed\u0026#39;) ax.set_ylabel(\u0026#39;# of genes\u0026#39;) ax.set_yscale(\u0026#39;symlog\u0026#39;) ax.set_title(\u0026#39;Gene detection across cells\u0026#39;) fig.tight_layout()   Here, around 16,000/35,000 genes are detected in fewer than 10 cells. We can remove these columns from the gene expression matrix moving forward.\nHow to remove lowly expressed genes using scprep. The syntax to remove these genes is similar to filtering on library size. The scprep function is scprep.filter.remove_rare_genes(). You can use it like this:\ndata = scprep.filter.remove_rare_genes(data, cutoff=0, min_cells=5) Another advantage of removing rare genes from our counts matrix is that it reduces the dimensions of the matrix that we\u0026rsquo;re working with. Instead of needing to do computations over a matrix that is 10,000x35,000, we are now working with one that is 10,000x16,000.\n1.5 - Library size normalization Why library size normalize? Now that we have our cells filtered by library size and have removed lowly expressed genes from our dataset, it\u0026rsquo;s time to normalize the data. Library size normalization is meant to align the scales of gene expression across cells that have different # of UMIs / cell. This is equivalent to comparing only the ratios of genes detected within a cell as opposed to comparing the absolute quantity of each RNA.\nMathematically, this process involves diving the count of each gene in each cell by the # UMIs in that cell. Optionally, one may then multiply the gene expression in all cells by a constant, such as the median # UMIs /cell in an experiment. This is purely a stylistic choice, but makes interpreting gene expression values a little easier.\nNormalizing library size using scprep This step is accomplished using the scprep.normalize.library_size_normalize() function with the following syntax:\ndata_ln = scprep.normalize.library_size_normalize(data) Here, I use data_ln to refer to data that has been library size normalized.\n1.6 - Removing cells with high mitochondrial gene expression What does high mitochondrial gene expression indicate? Generally, we assume that cells with high detection of mitochondrial RNAs have undergone degradation of the mitochondrial membrane as a result of apoptosis. This may be from stress during dissociation, culture, or really anywhere in the experimental pipeline. As with the high and low library size cells, we want to remove the long tail from the distribution. In a successful experiment, it\u0026rsquo;s typical for 5-10% of the cells to have this apoptotic signature.\nPlotting mitochondrial expression Let\u0026rsquo;s look at the mitochondrial expression from Datlinger et al. (2017). Here, the dashed line is the 95th percentile of mitochondrial expression.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  # get mitochondrial genes mitochondrial_gene_list = np.array([g.startswith(\u0026#39;MT-\u0026#39;) for g in data_ln.columns]) # get expression mito_exp = data_ln.loc[:,mitochondrial_gene_list].mean(axis=1) # plotting fig, ax = plt.subplots(1, figsize=(6,5)) ax.hist(mito_exp, bins=100) ax.axvline(np.percentile(mito_exp, 95)) ax.set_xlabel(\u0026#39;Mean mitochondrial expression\u0026#39;) ax.set_ylabel(\u0026#39;# of cells\u0026#39;) ax.set_title(\u0026#39;Mitochondrial expression\u0026#39;) fig.tight_layout()   Removing cells with high mitochondrial expression using scprep Each genome and species will have its own list of mitochondrial genes and associated symbols. In human and mouse, these genes start with \u0026lsquo;MT-\u0026rsquo;. However, as there\u0026rsquo;s no standard, we didn\u0026rsquo;t want to include a function for filtering mitochondrial expression directly in scprep. Instead, we provide the scprep.filter.filter_values() function. This method takes data and an array values and removes all cells from data where values is above or below the set threshold.\nHere, this would look like:\n1 2  # filter cells above 95th percentile data_ln = scprep.filter.filter_values(data_ln, mito_expression, percentile=95, keep_cells=\u0026#39;below\u0026#39;)   1.7 - Transforming data Our final step for preprocessing is to transform the data so that it\u0026rsquo;s usable for the algorithms that we\u0026rsquo;re using later.\nThe purpose of transforming data is to make sure that each gene or feature in our counts matrix is counted equally. Because of math, if we\u0026rsquo;re doing something like calculating the Euclidean distance between two cells, genes that are more highly expressed (i.e. have larger values) will be considered more important.\nThere are many transforms, but the two most common for scRNA-seq are the log-transform and the square-root transform. In CyTOF, the arcsinh transform is also popular. You can access all of these using scprep.transform.log(), scprep.transform.sqrt() or scprep.transform.arcsinh().\nOne note: the log-transform doesn\u0026rsquo;t like zeros, which are incredibly common in single cell datasets. To overcome this, people commonly add a pseudocount to their data, either 1 or a very small value called machine epsilon. Personally, I don\u0026rsquo;t like this because it skews the relationships between small values, which are a huge portion of single cell counts matrices.\nInstead, we use the squareroot transform 99% of the time. The syntax couldn\u0026rsquo;t be simpler:\ndata_sq = scprep.transform.sqrt(data_ln) We\u0026rsquo;re done preprocessing! Congratulations, we\u0026rsquo;re ready to start visualizing!\n","date":1536465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546318800,"objectID":"07f4e3ff19fbc6e7c4a88c1b56e99274","permalink":"https://dburkhardt.github.io/tutorial/preprocessing/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/tutorial/preprocessing/","section":"tutorial","summary":"1.0 Introducing scprep Scprep is a lightweight scRNA-seq toolkit for Python Data Scientists Most scRNA-seq toolkits are written in R, but we develop our tools in Python. Currently, Scanpy is the most popular toolkit for scRNA-seq analysis in Python. In fact, if you\u0026rsquo;d prefer to use that, you can find most of our lab\u0026rsquo;s analytical methods including PHATE and MAGIC in scanpy. However, scanpy has a highly structured framework for data representation and a steep learning curve that is unnescessary for users already comfortable with the suite of methods available in Pandas, Scipy, and Sklearn.","tags":null,"title":"1. Preprocessing","type":"docs"},{"authors":null,"categories":null,"content":" In this section, we\u0026rsquo;re going to go over a few introductory techniques for visualizing and exploring a single cell dataset. This is an essential step, and will tell us a lot about the nature of the data we\u0026rsquo;re working with. We\u0026rsquo;ll figure out things like:\n If the data exists on a trajectory, clusters, or a mix of both How many kinds of cells are likely present in a dataset If there are batch effects between samples If there are technical artifacts remaining after preprocessing  We\u0026rsquo;re going to use two main tools for this analysis: PCA and PHATE. PCA is useful because it\u0026rsquo;s quick and serves as a preliminary readout of what\u0026rsquo;s going on in a sample. However, PCA has many limitations as a visualization method because it can only recover linear combinations of genes. To get a better sense of the underlying structure of our dataset, we\u0026rsquo;ll use PHATE.\n2.0 - What is a visualization? Before we get too deep into showing a bunch of plots, I want to spend a little time discussing visualizations. Skip ahead if you want, but I think it\u0026rsquo;s important to understand what a visualization is, and what you can or cannot get from it.\nA visualization is a reduction of dimensions When we talk about data, we often consider the number of observations and the number of dimensions. In single cell RNA=-seq, the number of observations is the number of cells in a dataset. In other words, this is the number of rows. The number of dimensions, or number of features, is the number of genes. These are the columns in a gene expression matrix.\nIn a common experiment you might have 15,000-30,000 genes in a dataset measured across 5,000-100,000 cells. This presents a problem: How do you visually inspect such a dataset? The key is to figure out a way how to draw the relationships between points on a 2-dimensional sheet of paper, or if you add linear perspective, you can squeeze in a third dimension.\nA visualization is simply figuring out how to go from 30,000 dimensions -\u0026gt; 2-3.\nHeatmaps allow you to look at all genes across all cells simultaneously One way is to look at a heatmap. Here I\u0026rsquo;ve created a clustered heatmap from the Datlinger data using seaborn.clustermap:\nimport seaborn as sns cg = sns.clustermap(t_cell_data, cmap='inferno', xticklabels=[], yticklabels=[]) cg.ax_heatmap.set_xlabel('Genes ({})'.format(t_cell_data.shape[1])) cg.ax_heatmap.set_ylabel('Cells ({})'.format(t_cell_data.shape[0]))  It\u0026rsquo;s hard to draw any conclusions from this. How close together are any two cells? How do genes covary? We get some sense of this, and we are getting to look at all genes across all cells, but this representation of the data hinders hypothesis generation.\nBiplots show gene-gene relationships Another natural presentation is the biplot, commonly used for FACS analysis. Here each axis represents the expression of one of two genes and each dot is a cell. Let\u0026rsquo;s look at a biplot for some genes from the Datlinger dataset.\nAs you can see, it\u0026rsquo;s much easier to identify gene-gene relationships, but you can see how complex a plot we get when we look at only a handful of genes. Now realize that there are 312 million pairwise combinations of genes in a 25,000 gene genome.\nWe need a better solution.\nWhy can we reduce dimensions? In biological systems, we know that some genes are related to each other. These relationships are complex and nonlinear, but we do know that not all possible combinations of gene expression are valid.\nOn the left, points are uniformly distributed in the ambient 3-dimensional space. On the right, the points are randomly distributed on a 1-dimensional line that rolls in on itself. If we could unroll this line on the right, we would only need one or two dimensions to visualize it.\nHow can we reduce dimensions? There are many, many ways to visualize data. The most common ones are PCA, t-SNE, and MDS. Each of these has their own assumptions and simplifications they use to figure out an optimal 2D representation of high-dimensional data.\nPCA identifies linear combinations of genes such that each combination (called a Principal Component) that explains the maximum variance. t-SNE is a convex optimization algorithm that tries to minimize the divergence between the neighborhood distances of points (the distance between points that are \u0026ldquo;close\u0026rdquo;) in the low-dimensional representation and original data space.\nNeed intuitive explanation of MDS here  There are thousands of dimensionality reduction algorithms out there, and it\u0026rsquo;s important to understand that the drawbacks and benefits of each.\n2.1 - Visualizing data using PCA Why PCA? I like to start out any scRNA-seq by plotting a few principal components (PCs). First of all, we generally need to do PCA before doing any interesting downstream analysis, especially anything that involves graphs or matrices (Graphs are a mathematical representations of data as nodes and edges. More on that later).\nDoing multiplcation or inversion of matrices with 1,000+ dimensions gets very slow and takes up a lot of memory so we typically only store 100-500 PCs and use this for downstream analysis. PCA gives us a quantification of how much variance we\u0026rsquo;ve lost by removing some of the final principal components. This means we can figure out how many we need to capture 99.999% of the variance in a dataset.\nOnce we\u0026rsquo;ve calculated 100 PCs, we can just look at the first two to get a visualization.\nPCA dimensionality reduction using scprep By now you should know to expect that we\u0026rsquo;ve done the leg work here. You can easily perform PCA on any dataset using scprep.reduce.pca().\ndata_pcs = scprep.reduce.pca(data_sq, n_pca=100)  Now this simple syntax hides some complexity, so let\u0026rsquo;s dive a little deeper. If you don\u0026rsquo;t care, you can skip ahead to the \u0026ldquo;visualizing PCA section\u0026rdquo;.\nUsing the sklearn PCA operator Scikit-learn (sklearn) is a machine learning toolkit for Python. It\u0026rsquo;s excellent. Sklearn functions are the backbone of and stylistic-inspiration of production code in the lab. There\u0026rsquo;s an interesting paper on ArXiv about how sklearn is structured, and if you\u0026rsquo;re a machine learning programmer using Python, I\u0026rsquo;d recommend reading it.\nOne of sklearn\u0026rsquo;s fundamental units is the estimator class. One extremely useful estimator is the PCA class.\nYou can instantiate a PCA operator in one line:\nfrom sklearn.decomposition import PCA pc_op = PCA()  And fitting it to data is just as easy:\ndata_pcs = pc_op.fit_transform(data_sq)  Here we\u0026rsquo;re fitting the estimator to the data (i.e. calculating the principal component loadings) and then transforming it (i.e. projecting each point on those components).\nDuring fitting, information about the variance of each component is calculated and stores in the pc_op object as the explained_variance_ attribute. We can inspect this using a scree plot.\nfig, ax = plt.subplots(1, figsize=(6,5)) # plot explained variance as a fraction of the total explained variance ax.plot(pc_op.explained_variance_/pc_op.explained_variance_.sum()) # mark the 100th principal component ax.axvline(100, c='k', linestyle='--') ax.set_xlabel('PC index') ax.set_ylabel('% explained variance') ax.set_title('Scree plot') fig.tight_layout()  There are a few interesting things to note here. First, we see a distinct elbow point at around 50 PCs where the explained variance of each additional component drops significantly. This means that is we reduce the data to 100, 150, or 200 PCs, the marginal increase in explained variance is vanishingly smaller.\nSo is 100 a good cutoff? Let\u0026rsquo;s see how much variance is captured with this many components:\n\u0026gt;\u0026gt;\u0026gt; pc_op.explained_variance_.cumsum()[100]/pc_op.explained_variance_.sum() * 100 20.333394141561936  This means we\u0026rsquo;re capturing ~20% of the variance in these components. Beyond this point, we get a decreasing return in explained variance for each added component.\nHow to show that adding components doesn\u0026rsquo;t increase useful information 2.2 Visualizing PCA for exploratory analysis Now, I want to show why inspecting principal components is useful as a preliminary data analysis step. First, let\u0026rsquo;s consider the T-cell data from Datlinger et al.\nfig, ax = plt.subplots(1, figsize=(5,5)) ax.scatter(data_pcs[:,0], data_pcs[:,1], s=1) ax.set_xlabel('PC1') ax.set_ylabel('PC2') ax.set_title('T-cell - PCA') fig.tight_layout()  Here, each point here is a cells and the x and y axis represent the projection of that cell onto each principal component. Generally speaking, this PCA plot is unremarkable. There are a few outlier cells in the upper right with high PC2 loadings. Later we might want to look into what those cells, but for now its not so many that we\u0026rsquo;re very concerned about it.\nTo fully leverage the utility of PCA, let\u0026rsquo;s add a third dimension, color, to the plot. This way we can look at the distribution of library size, mitochondrial RNA, and cells from each sample.\nFirst, let\u0026rsquo;s look at the conditions:\nHere we can see that in the first two principal components, there is a good amount of overlap between the two conditions (stimulated and unstimulated). At this point, we can\u0026rsquo;t make many conclusions about the relationship between these conditions. We see that the range of cells states between samples is similar. We also observe that PC2 is associated somewhat with the condition label. Most importantly we don\u0026rsquo;t observe a batch effect separating the two samples.\n2.3 Identifying batch effects using PCA Compare the above plot to the following samples of mouse macrophage progenitors (unpublished). Here the first component visibily separates Sample 0 from Samples 1 \u0026amp; 2.\nThere is some mixing, but most of the cells in Sample 0 closest neighbors in the plot are all from the same sample. Compare this to Samples 1 \u0026amp; 2 where there are many cells from each sample that have both neighbors from Sample 1 and Sample 2.\nIn this dataset, we would say that there is some kind of batch effect between Sample 0 and Samples 1 \u0026amp; 2, but not between Samples 1 \u0026amp; 2. Note: I say batch effect here, but this does not mean that the shift is purely technical. In fact in this dataset, Sample 0 and Samples 1 \u0026amp; 2 are from different days of development.\n2.4 When separation doesn\u0026rsquo;t imply batch effect Let\u0026rsquo;s contrast this to a different dataset from from Shekhar, K. et al. (2018) Comprehensive Classification of Retinal Bipolar Neurons by Single-Cell Transcriptomics. Cell 166, 1308-1323.e30 (2016). Here, ~25,000 retinal bipolar cells were profiled using the Dropseq protocol. I picked this dataset because it profiles many different cell types from a terminally differentiated tissue, the adult mouse retina.\nHere, we also see multiple groups of cells, but in each cloud there is approximately equal representation of each of 6 replicates.\nRather, each cloud is associated with one or several annotated cell types from the paper.\n2.5 PCA confirms ordering of samples in a timecourse Finally, let\u0026rsquo;s consider samples from a single cell time course of human embryoid bodies (EBs) profiled in collaboration between our lab and the Ivanova laboratory at Yale. This dataset is described in the PHATE manuscript. This time course is a useful comparison to the Shekar Bipolar dataset because here we\u0026rsquo;re looking at a developing system that is also composed of many different lineages of stem, precursor, and progenitor cell types. This dataset is publicly available at Mendeley Data.\nHere we can see that the first principal component is tracking with the time of collection for each sample. The ordering of these samples matches the ordering of developmental time. This is expected and encouraging.\n2.6 Examining the distribution of library size Now, let\u0026rsquo;s look at the library size.\nThis plot shows us that PC1, the linear combination of genes with the highest variance, is strongly associated with library size. If we plot just those two factors, we can see the strength of that association.\nThis shows us that the log-library size is generating most of the variance in the dataset. This indicates that we might want to filter on library size more stringently. However, this is not highly unusual. We can keep this information in our back pockets for now, and move on.\nNow let\u0026rsquo;s look at the EB and retinal bipolar data:\nAgain, we see a similar trend between PC1 and library size. However, this isn\u0026rsquo;t always the case.\nIn this dataset we can see that there are cells in each point cloud with high library size. Now you might wonder, when can we see that there\u0026rsquo;s an issue by looking at PCs and library size?\n2.7 When principal components separate cells by library size The following dataset is from a published paper comparing mutant and wild-type conditions. Inspecting the data via PCA clearly shows two populations of cells separated by PC1. Unlike the previous example of batch effect, these populations contain an equal number of cells from each sample. This leads us to refer to this artifact as a within-batch effect.\nAs you can see, the distribution of library sizes in the right populations is much higher than on the left. In the next plot, I did KMeans on the first 100 PCs to extract the clusters. We\u0026rsquo;ll discuss clustering more later, but this hack worked to separate the populations so that we could look at the library size.\nNow this experiment wasn\u0026rsquo;t performed in our lab or by one of our collaborators so we\u0026rsquo;re not entirely sure what\u0026rsquo;s going on here. One guess is that the cells on the left are of poor quality and have less RNA. Another is that they are from a different batch than those on the right. We\u0026rsquo;ve reached out to the original lab that created this data, and they seemed as stumped as we were.\nAt this point, you might seriously consider throwing out the cells with lower library size.\n2.8 Examining mitochondrial RNA expression I don\u0026rsquo;t want to spend too much time on this, but just as we looked at library size above, you should plot mitochondrial RNA expression on a PCA plot.\nGenerally, you want the mitochondrial expression to look something like this:\nHere, the cells with the highest mitochondrial expression are generally evenly distributed on the plot. This is a good indication that you\u0026rsquo;ve done a good job filtering mitochondrial genes.\n","date":1536465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546318800,"objectID":"8cff0c015f314fa212fb9050239149ee","permalink":"https://dburkhardt.github.io/tutorial/visualizing_pca/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/tutorial/visualizing_pca/","section":"tutorial","summary":"In this section, we\u0026rsquo;re going to go over a few introductory techniques for visualizing and exploring a single cell dataset. This is an essential step, and will tell us a lot about the nature of the data we\u0026rsquo;re working with. We\u0026rsquo;ll figure out things like:\n If the data exists on a trajectory, clusters, or a mix of both How many kinds of cells are likely present in a dataset If there are batch effects between samples If there are technical artifacts remaining after preprocessing  We\u0026rsquo;re going to use two main tools for this analysis: PCA and PHATE.","tags":null,"title":"2. Visualizing PCA dimensions","type":"docs"}]